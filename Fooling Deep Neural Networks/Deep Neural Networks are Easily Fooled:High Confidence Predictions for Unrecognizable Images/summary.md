# Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images
http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf

## Related Works
1. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
   - Changing an image, originally correctly classified (e.g. as a lion), in a way imperceptible to human eyes, can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion as a library).
   - For example, Changing a few pixels in an image of lion can mislead the DNN to classify it as a library. Though the altered image seems like a lion to human, the DNN recognises it as a library.
2. D. Floreano and C. Mattiussi. Bio-inspired artificial intelligence:  theories,  methods,  and  technologies.   MIT  press,2008.
   - IDK (Need to read it)
3. A. Cully, J. Clune, and J.-B. Mouret. Robots that can adapt like natural animals. arXiv preprint arXiv:1407.3501, 2014.
   - IDK (Need to read it)
1. K. Deb. Multi-objective optimization using evolutionary algorithms, volume 16. John Wiley & Sons, 2001.
   - IDK (Need to read it)
5. H. Lipson. Principles of modularity, regularity, and hierarchy for scalable systems.Journal of Biological Physics andChemistry, 7(4):125, 2007.
   - IDK (Need to read it)
6. K. O. Stanley.  Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines, 8(2):131–162, 2007
   - Have a look here - https://github.com/ParikhKadam/genetic-or-evolutionary-algorithms#compositional-pattern-producing-networks-cppns
7. J. Secretan, N. Beato, D. B. D Ambrosio, A. Rodriguez, A. Campbell, and K. O. Stanley. Picbreeder: evolving pictures collaboratively online. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages1759–1768. ACM, 2008.
   - http://picbreeder.org/
   - A website which uses humans in order to decide the fitness of images generated by Evolutionary Algorithms
8. J. E. Auerbach. Automated evolution of interesting images. In Artificial Life 13, number EPFL-CONF-191282.MIT Press, 2012.
   - IDK (Need to read it)
9. K. Deb. Multi-objective optimization using evolutionary algorithms, volume 16. John Wiley & Sons, 2001.
10. I. Biederman. Visual object recognition, volume 2. MIT press Cambridge, 1995.
11. K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
12. D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network. Dept. IRO, Universit ́e de Montr ́eal, Tech. Rep, 2009.

## Introduction
It is easy to produce images that are completely unrecognizable to humans (Fig. 1), but that state-of-the-art DNNs believe to be recognizable objects with over 99% confidence (e.g. labeling with certainty that TV static (refers to black and white dots screen on TV) is a motorcycle).

|![Figure 1](images/fig1.jpg)

Figure 1

___

### Comparison with| [1]
This differs from [1] in a sense that, in [1] they modified the pixels of a lion image (the image contained a legit object) and the network misclassified it as library. Whereas, in| this paper|, the authors take a **garbage image** i.e. image with black and white dots (no legit object), and the **model classifies it as a motorcycle**.

Not only garbage image, but the authors also try on regular images, i.e. images with certain type of patterns as you will see below.

That is, the difference lies in image generation. [1] generates images by changing pixel values in meaningful images **(gradient ascent)** and those generated images contains the same meaning as before. Hence, for humans, it's like a very minor change but the DNNs are fooled. For DNNs, the object contained in the image has changed completely.

Whereas, in here, the authors generate images using evolutionary algorithms. There are two types of generated images: **irregular black and white dots** and **regular patterns like art**.

The authors also show image generation using gradient ascent. But it isn't the same approach as in [1].

___

We use **evolutionary algorithms** or **gradient ascent** to generate images.

We also find that, for MNIST DNNs, **it is not easy to prevent the DNNs from being fooled** by retraining them with fooling images labeled as such. While retrained DNNs learn to classify the negative examples as fooling images, anew batch of fooling images can be produced that fool these new networks, even after many retraining iterations.

Here, the authors experimented with MNIST dataset. They added a new class 10 for garbage images. Prepared a dataset of such garbage images to train the network on it, with an expectation that these are garbage images and stop misclassifying them. As expected, the network learned patterns (after all that's what the ML models do) and stopped misclassifying such images as legit target classes. But now, running the algorithm (mentioned above) with this newly trained model, it generated new garbage images which the model will misclassify. Repeating these steps, it showed no good results.

~In the above part, the authors did not add a new class. Instead, they expected the model to output low probability for each of the default classes. Thus the model effectively have a way to communicate "garbage" image. **The authors haven't yet tried adding a new "garbage" class.**~

To be clear, when retraining on garbage images, a new class is added. While, when generating garbage images **for the first time**, the authors expected the model to output low probabilities for each of real classes.

Also, the output probabilities of all classes must sum up to 1 i.e. the output activation function is softmax and not sigmoid.

### Topics discussed
1. Comparison between human vision and DNN-based computer vision
2. Performance of DNNs, in general, across different types of images than the ones they have been trained and traditionally tested on.

## Work

### Pre-trained models used
1. ImageNet DNN - AlexNet + ImageNet
2. MNIST DNN - LeNet + MNIST

### Generating images using evolutionary algorithm
EAs are optimization algorithms inspired by Darwinian evolution (the theory of evolution of humans -- from monkey to human). They contain a population of "organisms" (here, images) that alternately face **selection** (keeping the best) and then **random perturbation** (mutation and/or crossover). Which organisms are selected depends on the **fitness function**, which in these experiments is the highest prediction value a DNN makes for that image belonging to a class (Fig. 2).

To know more about these terms, visit the link of genetic algorithms provided in [For more information](#for-more-information) section of this file.

|![Figure 2](images/fig2.png)

Figure 2

The EA mentioned in [2|] optimize solutions to perform well on one objective or on a small set of objectives (e.g. evolving images to match a single ImageNet class).
|:--:|
|So,| we use a new algorithm called **the multi-dimensional archive of phenotypic elites MAP-Elites** [3], which enables us to simultaneously evolve a population that contains individuals that score well on many classes (e.g. all 1000 ImageNet classes).

**Fitness** is determined by showing the image to the DNN; if the image generates a higher prediction score (probability) **for any class** than has been seen before, the newly generated individual (image) becomes the champion in the archive **for that class**.

___

#### Representation of Images as genome (Image Encoding)
Two different types of encoding:
1. Direct Encoding
2. Indirect Encoding

**Direct Encoding**
- One grayscale integer for each of 28×28 pixels for MNIST, and three integers (H, S, V) for each of 256×256 pixels for ImageNet.
- Steps:
  1. Each pixel value is initialized with **uniform random noise** within the [0,255] range.
  2. Those numbers are independently mutated; first by determining which numbers are mutated, via a rate **(mutation rate)** that starts at 0.1 (each number has a 10% chance of being chosen to be mutated) and **drops by half every 1000 generations**.
  3. The numbers chosen to be mutated are then altered via the **polynomial mutation operator** [4] with a fixed **mutation strength of 15**.

**Indirect Encoding**
- Properties:
   1. More likely to produce regular images, i.e. images that contain compressible patterns (e.g. symmetry and repetition) [5].
   2. Indirectly encoded images tend to be regular because elements in the genome can affect multiple parts of the image [6].
- Specifically, the indirect encoding here is a **compositional pattern producing network (CPPN)** [6], which can evolve complex, regular images that resemble natural and man-made objects [7, 5, 8].
___

___

#### CPPNs
CPPN encoded EA can produce images that both humans and DNNs can recognize.

|![Figure 3](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig3.png)|
|:--:|
|Figure 3|

CPPNs are similar to artificial neural networks (ANNs). A CPPN takes in the(x, y) position of a pixel as input, and outputs a grayscale value (MNIST) or tuple of HSV color values (ImageNet) for that pixel. Like a neural network, the function the CPPN computes depends on the number of neurons in the CPPN, how they are connected, and the weights between neurons. Each CPPN node can be one of a set of activation functions (here: sine, sigmoid, Gaussian and linear), which can provide geometric regularities to the image. For example, passing the `x` input into a **Gaussian** function will provide left-right **symmetry**, and passing the `y` input into a **sine** function provides top-bottom **repetition**. **The topology, weights, and activation functions of each CPPN network in the population is determined by using genetic algorithm i.e evolution.**

To know more about CPPNs, visit the link of genetic algorithms provided in [For more information](#for-more-information) section of this file.

___

Above I said that, there are two types of generated images: **irregular black and white dots** and **regular patterns like art**. Now, let's relate them with image generation techniques. Images generated using direct encoding are irregular while the ones generated using indirect encoding, i.e. CPPN, are regular. 

## Results

### Fooling MNIST DNNs

#### Direct Encoding - Irregular Images
Multiple, independent runs of evolution repeatedly produce images that MNIST DNNs (such as LeNet) believe with 99.99% confidence to be digits, but are unrecognizable as such (Fig. 4). In less than 50 generations, each run of evolution repeatedly produces unrecognizable images of each digit type classified by MNIST DNNs with 99.99% confidence. **By 200 generations, median confidence is >= 99.99%.**

|![Figure 4](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig4.png)|
|:--:|
|Figure 4|

#### Indirect Encoding - Regular Images
Regular encoding might produce more recognizable images than the irregular white-noise static of the direct encoding. These images contain more strokes and other regularities, still the MNIST DNNs labels them as digits with 99.99% confidence (Fig. 5) after only a few generations. **By 200 generations, median confidence is 99.99%.**

|![Figure 5](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig5.png)|
|:--:|
|Figure 5|

**But**, certain patterns repeatedly evolve in some digit classes (Fig. 5). Images classified as a 1 tend to have vertical bars, while images classified as a 2 tend to have a horizontal bar in the lower half of the image. Qualitatively similar discriminative features are observed in 50 other runs as well. This result suggests that **the EA exploits specific discriminative features, corresponding to the handwritten digits, learned by MNIST DNNs**.

### Fooling ImageNet DNNs

The authors thought that MNIST DDNs got easily fooled because they are trained on a small dataset and hence, resulted in overfitting that data. To verify this, they experimented the same on ImageNet DNNs.

#### Direct Encoding - Irregular Images
Confidence scores for images were averaged over 10 crops (1 center, 4 corners and 5 mirrors) of size 227×227. The directly encoded EA was less successful at producing high-confidence images in this case. Even after 20,000 generations, **evolution failed to produce high-confidence images for many categories** (Fig. 6, median confidence 21.59%). However, evolution **did manage to produce images for 45 classes that are classified with >= 99% confidence** to be natural images (Fig. 1).

|![Figure 6](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig6.png)|
|:--:|
|Figure 6|

**While in some cases one might recognize features of the target class in the image, if told the class.** That means, the images generated were like, if you tell the class name to a human, he/she might identify features of that class in the image. But, if the class of that image is not disclosed, the human might not detect such features of that class in the image. Hence, if humans too can detect the features, there's no doubt that DNNs would recognize class in such irregular image.

#### Indirect Encoding - Regular Images
**In five independent runs, evolution produces many images with DNN confidence scores >= 99.99%, but that are unrecognizable** (Fig. 1 bottom). After 5000 generations, **the median confidence score reaches 88.11%**, similar to that for natural images (supplementary material) and  **significantly higher than the 21.59% for the direct encoding** High-confidence images are found in most categories (Fig. 7).

|![Figure 7](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig7.png)|
|:--:|
|Figure 7|

**While a human not given the class labels for CPPN images would not label them as belonging to that class, the generated images do often contain some features of the target class.** For example, in Fig. 1, the starfish image contains the blue of water and the orange of a starfish, the baseball has red stitching on a white background the remote control has a grid of buttons, etc. For many of the produced images, one can begin to identify why the DNN believes the image is of that class once given the class label. **This is because evolution need only to produce features that are unique to, or discriminative for, a class, rather than produce an image that contains all of the typical features of a class.**

**The authors observed that**:
1. EA is forced to produce the images which contain discriminative features of target class. That is, the images generated by EA contains the features of a class which differentiates it from other classes (Fig. 8). As a side note, one may also derive from this that, the DNNs observed here, only learnt the discriminative features of classes. That's obviously the difference between generative and discriminative models.

|![Figure 8](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig8.png)|
|:--:|
|Figure 8|

2. EA produces similar images for closely related categories (Fig. 9). For example, one image type receives high confidence scores for three types of lizards, and a different image type receives high confidence scores for three types of small, fluffy dogs.

|![Figure 9](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig9.png)|
|:--:|
|Figure 9|

3. Different runs of EA, produce different image types for these related categories. It reveals that there are different discriminative features per class that evolution exploits. That suggests that there are many different ways to fool the same DNN for each class.
4. Removing repeated patterns in images generated by CPPN, drops DNNs confidence score. CPPNs tend to produce regular images [6, 9]. Psychologists use the same ablation (removal) technique to learn which image features humans use to recognize objects [10]. In many images, ablating extra copies of the repeated element did lead to a performance drop, **albeit a small one** (Fig 10).

|![Figure 10](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig10.png)|
|:--:|
|Figure 10|

5. It means that the extra copies make the DNN more confident that the image belongs to the target class. This result is in line with a previous paper [26] that produced images to maximize DNN confidence scores, which also saw the emergence of features (e.g. a fox’s ears) repeated throughout an image.
6. These results suggest that DNNs tend to learn low-and middle-level features rather than the global structure of objects. If DNNs were properly learning global structure, images should receive lower DNN confidence scores if they contain repetitions of object subcomponents, because this rarely appears in natural images, such as many pairs of fox ears without nose in a single image.
7. The DNNs didn't get fooled for some classes in Fig. 7 (class numbers 157-286) i.e. the generated images got less confidence score for these classes. These classes are dogs and cats, which are overrepresented in the ImageNet dataset (i.e. there are many more classes of cats than classes of cars). There are two possible explanations for this:
   1. The network is tuned to identify many specific types of dogs and cats. Therefore, it ends up having more units dedicated to this image type than others. In other words, the size of the dataset of cats and dogs it has been trained on is larger than for other categories, meaning it is less overfit, and thus more difficult to fool.  If true, this explanation means that larger datasets are a way to ameliorate the problem of DNNs be-ing easily fooled.
   2. An alternate, though not mutually exclusive, explanation is that, because there are more cat and dog classes, the EA had difficulty finding an image that scores high in a specific dog category (e.g. Japanese spaniel), but low in any other related categories (e.g. Blenheim spaniel), which is necessary to produce a high confidence given that the final DNN layer is softmax. If it was sigmoid, the GA might have been successful in generating images, for which the DNNs give high confidence score for both the breeds of dog. But as it is softmax, the DNNs needs to distinguish between these breeds and hence, it becomes harder to GA to generate images which help DNNs to distinguish between these breeds. This explanation suggests that datasets with more classes can help ameliorate fooling.


### Comparison between DNNs
The results of the previous section suggest that there are discriminative features of a class of images that DNNs learn and evolution exploits. One question is whether different DNNs learn the same features for each class, or whether each trained DNN learns different discriminative features? One way to shed light on that question is to see if images that fool one DNN also fool another.

To test that, we evolved CPPN-encoded images with one DNN (DNN-A)and then input these images to another DNN (DNN-B). We tested two cases:
1. DNN-A and DNN-B have identical architectures and training, and differ only in their randomized weight initializations
2. DNN-A and DNN-B have different DNN architectures, but are trained on the same dataset.

We performed this test for both MNIST and ImageNet DNNs. Images were evolved that are given >= 99.99% confidence scores by bothDNN-A and DNN-B. Thus, some general properties of the DNNs are exploited by the CPPN-encoded EA. However, there are also images specifically fine-tuned to score high on DNN-A, but not on DNN-B. See the supplementary material for more detail and data.

### Retraining Networks to recognize fooling images
One might respond to the result that DNNs are easily fooled by saying that, while DNNs are easily fooled when images are optimized to produce high DNN confidence scores, the problem could be solved by simply changing the training dataset to include negative examples. In other words, a network could be retrained and told that the images that previously fooled it should not be considered members of any of the original classes, but instead should be recognized as a new “fooling images” class.

We tested that hypothesis with CPPN-encoded images on both MNIST and ImageNet DNNs. The process is as follows:
   - Train DNN1 on a dataset (e.g. ImageNet), then evolve CPPN images that produce a high confidence score for DNN1 for the `n` classes in the dataset
   - Take those images and add them to the dataset in a new class `n+1`
   - Train DNN2 on this enlarged "+1" dataset
   - Repeat the process (optional), but put the images that evolved for DNN2 in the `n+1` category (a `n+2` category is unnecessary because any images that fool a DNN are “fooling images” and can thus go in the `n+1` category).
  
Specifically, to represent different types of images, for each iteration, we add `m` images to this `n+1` category. These `m` images are randomly sampled from both the first and last generations of multiple runs of evolution that produce high confidence images for DNNi. Each evolution run on MNIST or ImageNet produces 20 and 2000 images respectively, with half from the first generation and half from the last. Error-rates for trained DNNi are similar to DNN1 (supplementary material).

#### MNIST DNNs vs ImageNet DNNs
The immunity of LeNet (MNIST DNN) is not boosted by retraining it with fooling images as negative examples. Evolution still produces many unrecognizable images for DNN2 with confidence scores of 99.99% (Fig. 11). For ImageNet models, evolution was less able to evolve high confidence images for DNN2 than DNN1. The median confidence score significantly decreased from 88.1% for DNN1 to 11.7% for DNN2 (Fig. 12, p<0.0001 via Mann-Whitney U test).

|![Figure 11](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig11.png)|
|:--:|
|Figure 11|

|![Figure 12](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig12.png)|
|:--:|
|Figure 12|

We suspect that ImageNet DNNs were better inoculated against being fooled than MNIST DNNs when trained with negative examples because it is easier to learn to tell CPPN images apart from natural images than it is to tell CPPN images from MNIST digits. **Though I do not believe this..**

To see whether this DNN2 had learned features specific to the CPPN images that fooled DNN1, or whether DNN2 learned features general to all CPPN images, even recognizable ones, we input recognizable CPPN images from Picbreeder.org to DNN2. These are the images that DNN2 isn't trained on. DNN2 correctly labeled 45 of 70 (64%, top-1 prediction) PicBreeder images as CPPN images, despite having never seen CPPN images like them before. **The retrained model thus learned features generic to CPPN images, helping to explain why producing new images that fool DNN2 is more difficult.**

### Generating fooling images via gradient ascent
A different way to produce high confidence, yet mostly unrecognizable images is by using gradient ascent in pixel space [12, 11, 1]. We calculate the gradient of the posterior probability for a specific class - here, a softmax output unit of the DNN - with respect to the input image using backprop, and then we follow the gradient to increase a chosen unit’s activation. This technique follows [11], but whereas we aim to find images that produce high confidence classifications, they sought visually recognizable "class appearance models". By employing L2-regularization, they produced images with some recognizable features of classes (e.g. dog faces, fox ears, and cup handles). However, their confidence values are not reported, so to determine the degree to which DNNs are fooled by these backpropagated images, we replicated their work (with some minor changes, see supplementary material) and found that images can be made that are also classified by DNNs with 99.99% confidence, despite them being mostly unrecognizable (Fig. 13). These optimized images reveal a third method of fooling DNNs that produces qualitatively different examples than the two evolutionary methods in this paper.

|![Figure 13](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig13.png)|
|:--:|
|Figure 13|

## For more information
1. Official Website - http://www.evolvingai.org/fooling
2. Genetic/Evolutionary Algorithms - https://github.com/ParikhKadam/genetic-or-evolutionary-algorithms
3. PicBreeder - http://picbreeder.org/
4. Sferes evolutionary computation framework - https://github.com/sferes2/sferes2 

## Topics to look at
1. Mann-Whitney U test

## Things I learned on how to read a paper
1. If you can't understand the process, just skip that part for a while. Process is only important when you want to implement it.
2. Observations/Results are the most important part of a paper. They are also the most interesting.

## English Vocabulary
1. albeit - though
2. ablation - removal
3. ameliorate - make (something bad or unsatisfactory) better
4. inoculate - vaccinate