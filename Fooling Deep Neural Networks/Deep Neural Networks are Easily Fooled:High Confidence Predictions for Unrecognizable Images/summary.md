## Related Works
1. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
  - Changing an image, originally correctly classified (e.g. as a lion), in a way imperceptible to human eyes, can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion as a library).
  - For example, Changing a few pixels in an image of lion can mislead the DNN to classify it as a library. Though the altered image seems like a lion to human, the DNN recognises it as a library.
2. D. Floreano and C. Mattiussi. Bio-inspired artificial intelligence:  theories,  methods,  and  technologies.   MIT  press,2008.
  - IDK (Need to read it)
3. A. Cully, J. Clune, and J.-B. Mouret. Robots that can adapt like natural animals. arXiv preprint arXiv:1407.3501, 2014.
  - IDK (Need to read it)
4. K. Deb. Multi-objective optimization using evolutionary algorithms, volume 16. John Wiley & Sons, 2001.
  - IDK (Need to read it)
5. H. Lipson. Principles of modularity, regularity, and hierarchy for scalable systems.Journal of Biological Physics andChemistry, 7(4):125, 2007.
  - IDK (Need to read it)
6. K. O. Stanley.  Compositional pattern producing networks: A novel abstraction of development. Genetic programming and evolvable machines, 8(2):131–162, 2007
  - Have a look here - https://github.com/ParikhKadam/genetic-or-evolutionary-algorithms#compositional-pattern-producing-networks-cppns
7. J. Secretan, N. Beato, D. B. D Ambrosio, A. Rodriguez, A. Campbell, and K. O. Stanley. Picbreeder: evolving pictures collaboratively online. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages1759–1768. ACM, 2008.
  - http://picbreeder.org/
  - A website which uses humans in order to decide the fitness of images generated by Evolutionary Algorithms
8. J. E. Auerbach. Automated evolution of interesting images. In Artificial Life 13, number EPFL-CONF-191282.MIT Press, 2012.
  - IDK (Need to read it)
9. ...

## Introduction
It is easy to produce images that are completely unrecognizable to humans (Fig. 1), but that state-of-the-art DNNs believe to be recognizable objects with over 99% confidence (e.g. labeling with certainty that TV static (refers to black and white dots screen on TV) is a motorcycle).

![Figure 1](images/fig1.jpg)

Figure 1

___

### Comparison with [1]
This differs from [1] in a sense that, in [1] they modified the pixels of a lion image (the image contained a legit object) and the network misclassified it as library. Whereas, in this paper, the authors take a **garbage image** i.e. image with black and white dots (no legit object), and the **model classifies it as a motorcycle**.

Not only garbage image, but the authors also try on regular images, i.e. images with certain type of patterns as you will see below.

That is, the difference lies in image generation. [1] generates images by changing pixel values in meaningful images **(gradient ascent)** and those generated images contains the same meaning as before. Hence, for humans, it's like a very minor change but the DNNs are fooled. For DNNs, the object contained in the image has changed completely.

Whereas, in here, the authors generate images using evolutionary algorithms. There are two types of generated images: **irregular black and white dots** and **regular patterns like art**.

___


We use **evolutionary algorithms** or **gradient ascent** to generate images.

We also find that, for MNIST DNNs, **it is not easy to prevent the DNNs from being fooled** by retraining them with fooling images labeled as such. While retrained DNNs learn to classify the negative examples as fooling images, anew batch of fooling images can be produced that fool these new networks, even after many retraining iterations.

Here, the authors experimented with MNIST dataset. ~They added a new class 10 for garbage images.~ Prepared a dataset of such garbage images to train the network on it, with an expectation that these are garbage images and stop misclassifying them. As expected, the network learned patterns (after all that's what the ML models do) and stopped misclassifying such images as legit target classes. But now, running the algorithm (mentioned above) with this newly trained model, it generated new garbage images which the model will misclassify. Repeating these steps, it showed no good results.

In the above part, the authors did not add a new class. Instead, they expected the model to output low probability for each of the default classes. Thus the model effectively have a way to communicate "garbage" image. **The authors haven't yet tried adding a new "garbage" class.**

Also, the output probabilities of all classes must sum up to 1 i.e. the output activation function is softmax and not sigmoid.

### Topics discussed
1. Comparison between human vision and DNN-based computer vision
2. Performance of DNNs, in general, across different types of images than the ones they have been trained and traditionally tested on.

## Work

### Pre-trained models used
1. ImageNet DNN - AlexNet + ImageNet
2. MNIST DNN - LeNet + MNIST

### Generating images using evolutionary algorithm
EAs are optimization algorithms inspired by Darwinian evolution (the theory of evolution of humans -- from monkey to human). They contain a population of "organisms" (here, images) that alternately face **selection** (keeping the best) and then **random perturbation** (mutation and/or crossover). Which organisms are selected depends on the **fitness function**, which in these experiments is the highest prediction value a DNN makes for that image belonging to a class (Fig. 2).

To know more about these terms, visit the link of genetic algorithms provided in [For more information](#for-more-information) section of this file.

![Figure 2](images/fig2.png)

Figure 2

The EA mentioned in [2] optimize solutions to perform well on one objective or on a small set of objectives (e.g. evolving images to match a single ImageNet class).

So, we use a new algorithm called **the multi-dimensional archive of phenotypic elites MAP-Elites** [3], which enables us to simultaneously evolve a population that contains individuals that score well on many classes (e.g. all 1000 ImageNet classes).

**Fitness** is determined by showing the image to the DNN; if the image generates a higher prediction score (probability) **for any class** than has been seen before, the newly generated individual (image) becomes the champion in the archive **for that class**.

___

#### Representation of Images as genome (Image Encoding)
Two different types of encoding:
1. Direct Encoding
2. Indirect Encoding

**Direct Encoding**
- One grayscale integer for each of 28×28 pixels for MNIST, and three integers (H, S, V) for each of 256×256 pixels for ImageNet.
- Steps:
  1. Each pixel value is initialized with **uniform random noise** within the [0,255] range.
  2. Those numbers are independently mutated; first by determining which numbers are mutated, via a rate **(mutation rate)** that starts at 0.1 (each number has a 10% chance of being chosen to be mutated) and **drops by half every 1000 generations**.
  3. The numbers chosen to be mutated are then altered via the **polynomial mutation operator** [4] with a fixed **mutation strength of 15**.

**Indirect Encoding**
- Properties:
   1. More likely to produce regular images, i.e. images that contain compressible patterns (e.g. symmetry and repetition) [5].
   2. Indirectly encoded images tend to be regular because elements in the genome can affect multiple parts of the image [6].
- Specifically, the indirect encoding here is a **compositional pattern producing network (CPPN)** [6], which can evolve complex, regular images that resemble natural and man-made objects [7, 5, 8].
___

___

#### CPPNs
CPPN encoded EA can produce images that both humans and DNNs can recognize.

![Figure 3](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig3.png)

Figure 3

CPPNs are similar to artificial neural networks (ANNs). A CPPN takes in the(x, y) position of a pixel as input, and outputs a grayscale value (MNIST) or tuple of HSV color values (ImageNet) for that pixel. Like a neural network, the function the CPPN computes depends on the number of neurons in the CPPN, how they are connected, and the weights between neurons. Each CPPN node can be one of a set of activation functions (here: sine, sigmoid, Gaussian and linear), which can provide geometric regularities to the image. For example, passing the `x` input into a **Gaussian** function will provide left-right **symmetry**, and passing the `y` input into a **sine** function provides top-bottom **repetition**. **The topology, weights, and activation functions of each CPPN network in the population is determined by using genetic algorithm i.e evolution.**

To know more about CPPNs, visit the link of genetic algorithms provided in [For more information](#for-more-information) section of this file.

___

Above I said that, there are two types of generated images: **irregular black and white dots** and **regular patterns like art**. Now, let's relate them with image generation techniques. Images generated using direct encoding are irregular while the ones generated using indirect encoding are regular. 

## Results

### Fooling MNIST DNNs

#### Direct Encoding - Irregular Images
Multiple, independent runs of evolution repeatedly produce images that MNIST DNNs (such as LeNet) believe with 99.99% confidence to be digits, but are unrecognizable as such (Fig. 4). In less than 50 generations, each run of evolution repeatedly produces unrecognizable images of each digit type classified by MNIST DNNs with 99.99% confidence. By 200 generations, median confidence is >= 99.99%.

![Figure 4](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig4.png)

Figure 4

#### Indirect Encoding - Regular Images
Regular encoding might produce more recognizable images than the irregular white-noise static of the direct encoding. These images contain more strokes and other regularities, still the MNIST DNNs labels them as digits with 99.99% confidence (Fig. 5) after only a few generations. By 200 generations, median confidence is 99.99%.

![Figure 5](/Fooling&#32;Deep&#32;Neural&#32;Networks/Deep&#32;Neural&#32;Networks&#32;are&#32;Easily&#32;Fooled:High&#32;Confidence&#32;Predictions&#32;for&#32;Unrecognizable&#32;Images/images/fig5.png)

Figure 5

**But**, certain patterns repeatedly evolve in some digit classes (Fig. 5). Images classified as a 1 tend to have vertical bars, while images classified as a 2 tend to have a horizontal bar in the lower half of the image. Qualitatively similar discriminative features are observed in 50 other runs as well. This result suggests that **the EA exploits specific discriminative features, corresponding to the handwritten digits, learned by MNIST DNNs**.

## For more information
1. Official Website - http://www.evolvingai.org/fooling
2. Genetic/Evolutionary Algorithms - https://github.com/ParikhKadam/genetic-or-evolutionary-algorithms
3. PicBreeder.org - http://picbreeder.org/
4. Sferes evolutionary computation framework - https://github.com/sferes2/sferes2 